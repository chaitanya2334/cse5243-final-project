{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![osu logo](./images/osu-32px-horiz.png)\n",
    "\n",
    "<hr size=\"6\" style=\"color:#d65828;background-color:#d65828;border:none;\">\n",
    "\n",
    "# Project: Deep Learning\n",
    "\n",
    "This laboratory assignment was designed by Chaitanya Kulkarni and Raghu Machiraju.\n",
    "\n",
    "**Due Date:** April 30, 2020, 23:59:59. No late submissions allowed unless the Night King from Game of Thrones arises again and invades our campus. Please consult the instructor if you are falling behind.\n",
    "\n",
    "Some salient points:\n",
    "* The total number of points is 300 (30% of total grade).\n",
    "* The distribution for each part and question is listed below.\n",
    "* Teams of two/three enrolled students will complete this project.\n",
    "* Discussion across teams is only permitted on piazza.\n",
    "* Teams are forbidden to exchange code.\n",
    "* If any code is found and used, it should be mentioned in the notebook.\n",
    "* Submission will be through Carmen and we will accept only Jupyter notebooks.\n",
    "* It is required that the notebooks are zipped up and deposited.\n",
    "* The zip file should be named in the following way lastname1-lastame2-labN. (1<=N<=4).\n",
    "* Please please do not include the data with the .zip file.\n",
    "* It is very important that the class follows the honor code of academic conduct to maximize learning in an open manner.\n",
    "\n",
    "\n",
    ">### _Objective of this Lab:_\n",
    "> * **(200 pts) Part 1** - Build an ANN: Realize a simple, L-layer artificial neural network with both forward and backward propagation. Understand how to train, visualize and evaluate the results of the network on the MNIST dataset.\n",
    ">\n",
    "> * **(100 pts) Part 2** - Build your best deep learning (DL) model: Learn to construct your own CNN architectures using Pytorch modules. Learn how to separate a validation set and use it to find the best set of hyperparameters and to also determine when to stop training any given deep learning or machine learning models. \n",
    "\n",
    "<hr size=\"6\" style=\"color:#d65828;background-color:#d65828;border:none;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Build an Artificial Neural Network \n",
    "\n",
    "Here you will build and train an ANN from scratch to predict the 10 class labels from N image points in a 784 dimensional space (note that each is image 28x28 or 784 pixels). Each pixel will take a value ranging from 0-255. Boilerplate starter code will be provided. As before you only have to fill in the empty code blocks. Additional details are provided within each function call and definitions. We breakdown the process into 4 steps which are also laid out in the starter code.\n",
    "\n",
    "# Step 1: Build the HiddenLayer Class\n",
    "\n",
    "Create a HiddenLayer class such that:\n",
    "1. Stores the weights/ parameters for that layer;\n",
    "2. Defines a forward function to update weights of the current layer at each step;\n",
    "3. Support activation functions that also calculate respective gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer:\n",
    "    def __init__(self, weights_shape, bias=True, activation=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        Defines a HiddenLayer that performs a linear transformation\n",
    "        with the learnt weights. Supports a collection of activation functions.\n",
    "        \n",
    "        :param weights_shape: shape of the weights defined as (in_features, out_features).\n",
    "        :param bias: whether or not to use the bias term in feed forward calculation.\n",
    "        :param activation: activation function to use.\n",
    "        \"\"\"\n",
    "        #code here\n",
    "        pass\n",
    "        \n",
    "    def sigmoid(self, x, d=False):\n",
    "        \"\"\"\n",
    "        The sigmoid activation function with support for derviatives. \n",
    "        \"\"\"\n",
    "        # code here\n",
    "        if d:\n",
    "            # return derivative of sigmoid\n",
    "            pass\n",
    "        \n",
    "        # return sigmoid of x\n",
    "        pass\n",
    "    \n",
    "    def tanh(self, x, d=False):\n",
    "        \"\"\"\n",
    "        The tanh activation function with support for derviatives.\n",
    "        \"\"\"\n",
    "        #code here\n",
    "        \n",
    "        if d:\n",
    "            # return derivative of tanh\n",
    "            pass\n",
    "        \n",
    "        # return tanh of x.\n",
    "        pass\n",
    "    \n",
    "    # define more activation functions here (ReLU, linear, etc)\n",
    "            \n",
    "    def init_weights(self, weights_shape):\n",
    "        \"\"\"\n",
    "        Defines how to initialize the weights. Random or all zeros.\n",
    "        :param weights_shape: the dimensions of the weight matrix to initialize.\n",
    "        \"\"\"\n",
    "        # code here\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Implement the feed forward equation here. \n",
    "        Be sure to store the input and output as those will be \n",
    "        needed in back propogation part.\n",
    "        \n",
    "        :param X: Input of size (m, d), \n",
    "            where m is the batch size, and d is the input feature dim \n",
    "            coming from the previous layer.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.out = None # calculate feed forward for the given layer.\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Build the Network with Back Propagation \n",
    "\n",
    "Use **_Mean Squared Error_** as your loss/cost function. For bonus points implement the **_Cross Entropy_** *as another candidate for a loss function.* \n",
    "\n",
    "To support back propogation, revisit the following puesdo code:\n",
    "\n",
    "> #### &emsp;For every mini-batch $X$ of size $m$:\n",
    ">\n",
    "> &emsp;&emsp;1. Generate the Output error $\\delta_L$ of shape $(m, L)$:\n",
    ">    \n",
    "> &emsp;&emsp;&emsp;&emsp;$\\delta_L = \\Delta \\odot \\sigma^\\prime_L(z_L)$\n",
    ">    \n",
    "> &emsp;&emsp;2. For every remaining layers $(L - 1, L - 2, ... 1)$ calculate $\\delta_l$ of shape $(m, l)$:\n",
    ">    \n",
    "> &emsp;&emsp;&emsp;&emsp;$\\delta_l = (w_{l+1} \\cdot \\delta_{l+1}) \\odot \\sigma^\\prime_l(z_l)$\n",
    ">    \n",
    "> &emsp;&emsp;3. Perform gradient descent by updating the weights and bias of all the layers:\n",
    ">    \n",
    "> &emsp;&emsp;&emsp;&emsp;$w_l \\rightarrow w_l - \\frac{\\eta}{m} \\sum_{x}\\delta_{x,l}a_{x,l-1}^T$\n",
    ">    \n",
    "> &emsp;&emsp;&emsp;&emsp;$b_l \\rightarrow b_l - \\frac{\\eta}{m} \\sum_{x}\\delta_{x,l}$\n",
    "\n",
    "Where,\n",
    "\n",
    "$m$ - Batch size\n",
    "\n",
    "$x, X$ - are a single sample and a batch of samples respectively\n",
    "\n",
    "$\\Delta$ - Is the cost function's derivative\n",
    "\n",
    "$\\odot$ - [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) which realizes element-wise matrix multiplication (see below for illustration)\n",
    "\n",
    "$L$ - Is the last layer\n",
    "\n",
    "$\\eta$ - learning rate\n",
    "\n",
    "$\\sigma$ - activation function\n",
    "\n",
    "$a, z, b, w$ - are defined by the given layer $l$'s feed forward equation: $z_{l+1} = w_{l+1} \\cdot a_l + b_{l+1}$ \n",
    "\n",
    "The Hadamard product operates on identically shaped matrices and produces a third matrix of the same dimensions. \n",
    "\n",
    "<img style=\"float: left;\" src=https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Hadamard_product_qtl1.svg/1200px-Hadamard_product_qtl1.svg.png width=\"400\">\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "class ANN:\n",
    "    def __init__(self, arch, lr, input_shape):\n",
    "        \"\"\"\n",
    "        Build the ANN network based on the architecture specified.\n",
    "        \n",
    "        \n",
    "        :param arch: `arch` is a list of values specifying the number \n",
    "            of nodes in each hidden layer. \n",
    "            ie arch = [200, 100, 10] will create a 3-layer network with the \n",
    "            first layer containing 200 nodes, second 100, and the final 10.\n",
    "        :param lr: the learning rate\n",
    "        :input_shape: A tuple that contains the expected input shape. \n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.layers = []\n",
    "        self.lr = lr\n",
    "        self.build(arch)\n",
    "            \n",
    "    def build(self, arch):\n",
    "        \"\"\"\n",
    "        Builds the network arch and store the hidden layers in self.layers.\n",
    "        \n",
    "        :param arch: specify the number of nodes in each hidden \n",
    "            layer as a list where the length is the number of layers.\n",
    "            \n",
    "        :return : none\n",
    "        \"\"\"\n",
    "        for nb_nodes in arch:\n",
    "            # code here\n",
    "            layer = None # build hidden layer here\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Run the input through each layer's forward implementation.\n",
    "        Return the final layer's output.\n",
    "        \n",
    "        :param X: Input as a matrix of size (m, 784)\n",
    "        \n",
    "        :return out: final layers output\n",
    "        \"\"\"\n",
    "        # code here\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        Implement the back propogation algorithm here. Make use of \n",
    "        each layer's stored input's and outputs to update each layer's weights and biases.\n",
    "        \n",
    "        :param y: target labels as a matrix of size (m, 10).\n",
    "        \n",
    "        :return : none\n",
    "        \"\"\"\n",
    "        \n",
    "        # code here\n",
    "        pass\n",
    "            \n",
    "    def mse(self, y_pred, y, d=False):\n",
    "        \"\"\"\n",
    "        Calculate the loss/cost function. Also define its derivative.\n",
    "        :param y_pred: last layer's output after feed forward. Matrix of size (m, 10)\n",
    "        :param y: target labels as a matrix of size (m, 10)\n",
    "        \n",
    "        :return mse: return either the cost function, or its derivative.\n",
    "        \"\"\"\n",
    "        \n",
    "        # code here\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def train(self, train_loader):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        :param train_loader: an iterator that generates mini-batches for train data\n",
    "        \n",
    "        :return all_loss: a list of loss values per mini-batch. (to plot)\n",
    "        \"\"\"\n",
    "        all_loss = []\n",
    "        for X, y in train_loader:\n",
    "            # code here\n",
    "            pass\n",
    "            \n",
    "        return all_loss\n",
    "    \n",
    "    def test(self, test_loader):\n",
    "        \"\"\"\n",
    "        Test the model. \n",
    "        You can also return the accuracy scores here to see \n",
    "        if the model is training over each epoch.\n",
    "        \n",
    "        :param test_loader: an iterator that generates mini-batches for test data\n",
    "        \n",
    "        :return all_loss: a list of loss values per mini-batch. (to plot)\n",
    "        :return acc: accuracy score for the full 10k test dataset. (to plot)\n",
    "        \"\"\"\n",
    "        loss = []\n",
    "        for X, y in test_loader:\n",
    "            # code here\n",
    "            pass\n",
    "            \n",
    "        return loss, acc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train on MNIST Dataset\n",
    "\n",
    "Find the MNIST PreProcessed Dataset at  https://www.kaggle.com/oddrationale/mnist-in-csv. \n",
    "\n",
    "We shall use the `mnist_train.csv` (60k) for training, and `mnist_test.csv` (10k) for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv(\"data/mnist_train.csv\")\n",
    "train_y = train_df['label'].to_numpy()\n",
    "train_X = train_df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "test_df = pd.read_csv(\"data/mnist_test.csv\")\n",
    "test_y = test_df['label'].to_numpy()\n",
    "test_X = test_df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "def get_batch(df, n=64):\n",
    "    y = pd.get_dummies(df['label']).to_numpy()\n",
    "    X = df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "    for i in range(X.shape[0] // n):\n",
    "        yield X[n*i:n*(i+1)], y[n*i:n*(i+1)]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NB_EPOCHS = 1000\n",
    "\n",
    "# example ANN\n",
    "net = ANN([100, 10], lr=0.002, input_shape=(BATCH_SIZE, train_X.shape[1]))\n",
    "\n",
    "# train and test the model over multiple epoch.\n",
    "for i in range(NB_EPOCHS):\n",
    "    train_loader = get_batch(train_df, n=BATCH_SIZE)\n",
    "    test_loader = get_batch(test_df, n=BATCH_SIZE)\n",
    "    train_loss = net.train(train_loader)\n",
    "    test_loss = net.test(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluate ANN\n",
    "\n",
    "* Generate confusion matrix, and accuracy score for the test set\n",
    "* Update the ANN class to generate live plots of loss function as the model trains. Use `LivePlotter` provided below.\n",
    "* Now experiment with the following hyper parameters and report your results.\n",
    "    * Network Architecture (number of layers, number of nodes in each layer, weight initializations, etc)\n",
    "    * Batch size\n",
    "    * Learning rate\n",
    "    * (Bonus) Try **_Cross Entropy_** cost function and compare against **_Mean Squared Error_**.\n",
    "* Report your observations. Which set of hyper parameters worked the best. Why ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr size=\"6\" style=\"color:#d65828;background-color:#d65828;border:none;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Build the Best Model for MNIST\n",
    "\n",
    "n this part, you can construct your **own Convolutional Neural Network (CNN) model** using [Pytorch](https://pytorch.org/) modules and experiment with different types of deep learning models you may find in literature (see below), or that you conceived on your own. It is **important** to reference any papers/books you used to realize or develop your models. \n",
    "\n",
    "\n",
    "Otherwise, you are also free to modify the provided CNN architecture which is described below. Separate a validation set for the MNIST data and use it to find the best set of hyperparameters and determine when to stop training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build the Model\n",
    "\n",
    "A standard 2-layer CNN architecture is provided . Experiment with different types of models you may find in literature, or that you conceived on your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Live Plotting\n",
    "\n",
    "This class will allow you to display plots as you are training the model. By default it is set to update plots every 20 sec. You are free to make any modifications to this class if you need additional support to display other than just line plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "class LivePlotter:\n",
    "    def __init__(self, title='', plot_every_n_sec=20):\n",
    "        self.data_dict = defaultdict(list)\n",
    "        self.global_step = 0\n",
    "        self.plot_every_n_sec = plot_every_n_sec\n",
    "        self.start_time = time.time()\n",
    "        self.figsize = (7, 5)\n",
    "        self.title = title\n",
    "        self.plot_idx = {}\n",
    "        self.fig = None\n",
    "        \n",
    "    def update_step(self, step):\n",
    "        \"\"\"\n",
    "        increment global step by `step`.\n",
    "        \"\"\"\n",
    "        self.global_step += step\n",
    "        \n",
    "    def update(self, data_type, y):\n",
    "        \"\"\"\n",
    "        Add a datapoint and plot periodically. \n",
    "        Datapoint is added to the data dictionary given the key in `data_type`.\n",
    "        :param data_type:\n",
    "        :param y: datapoint to plot\n",
    "        \"\"\"\n",
    "        self.data_dict[data_type].append((self.global_step, y))\n",
    "        \n",
    "        # plot periodically\n",
    "        if time.time() - self.start_time > self.plot_every_n_sec:\n",
    "            self.plot()\n",
    "            self.start_time = time.time()\n",
    "    \n",
    "    def plot(self):\n",
    "        if self.fig:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        for label, data in self.data_dict.items():\n",
    "            if label not in self.plot_idx:\n",
    "                self.plot_idx[label] = len(self.plot_idx) + 1\n",
    "       \n",
    "        self.fig = plt.figure(figsize=(7*len(self.plot_idx), 5))\n",
    "        \n",
    "        for label, data in self.data_dict.items():    \n",
    "            axs = plt.subplot(1, len(self.plot_idx), self.plot_idx[label])\n",
    "            x, y = zip(*data)\n",
    "            axs.plot(x, y, label=label)\n",
    "        \n",
    "            plt.title(label)\n",
    "            plt.grid(True)\n",
    "            plt.xlabel('global_step')\n",
    "            plt.legend(loc='center left') \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement early stopping\n",
    "\n",
    "Implement end stopping using the validation set. Your model should stop training if you do not observe any improvement on the validation set. Set `early_stopping` as one of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# if you have an nvidia gpu you can improve your run speeds, \n",
    "# otherwise its not necessary for this project.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "class TrainingFramework:\n",
    "    def __init__(self, train_loader, test_loader, live_plotter=None, max_epochs=100, lr=0.2):\n",
    "        \"\"\"\n",
    "        Initialize training framework with the model, optimizer, the data loaders and\n",
    "        all the relevant hyperparameters. \n",
    "        \n",
    "        :param train_loader: DataLoader for training dataset\n",
    "        :param test_loader: DataLoader for test dataset\n",
    "        :param live_plotter: LivePlotter instance to be used to update plots over training/testing\n",
    "        :param max_epochs: max number of epochs to train\n",
    "        :param lr: learning rate\n",
    "        \"\"\"\n",
    "        self.model = Net().to(device)\n",
    "        self.optimizer = optim.Adadelta(self.model.parameters(), lr=lr)\n",
    "        self.max_epochs = max_epochs\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        if live_plotter is None:\n",
    "            self.live_plotter = LivePlotter(\"Plottn' Live\", plot_every_n_sec=30)\n",
    "        else:\n",
    "            self.live_plotter = live_plotter\n",
    "                \n",
    "\n",
    "    def train(self, epoch):\n",
    "        \"\"\"\n",
    "        Core training function. Performs a single training session in one epoch.\n",
    "        Leave unmodified.\n",
    "        \n",
    "        :param epoch: current epoch number\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        for batch_idx, (X, y) in enumerate(tqdm(self.train_loader, desc=\"Training\")):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            loss = self.train_batch(batch_idx, (X, y))\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # collect relevant datapoints to plot\n",
    "            self.live_plotter.update_step(len(X))\n",
    "            self.live_plotter.update('train_loss', loss.item())\n",
    "\n",
    "    def test(self, epoch):\n",
    "        \"\"\"\n",
    "        Core testing function. Performs a single testing session in one epoch.\n",
    "        Leave unmodified.\n",
    "        \n",
    "        :param epoch: current epoch number\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (X, y) in enumerate(tqdm(self.test_loader, desc=\"Testing\")):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                \n",
    "                loss, pred = self.test_batch(batch_idx, (X, y))\n",
    "                \n",
    "                test_loss += loss\n",
    "                correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "                \n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "\n",
    "        # collect relevant datapoints to plot\n",
    "        self.live_plotter.update_step(len(self.test_loader.dataset))\n",
    "        self.live_plotter.update(data_type='test_loss', y=loss)\n",
    "        self.live_plotter.update(data_type='test_acc', \n",
    "                                 y=correct/len(test_loader.dataset))\n",
    "        \n",
    "    def dev(self):\n",
    "        \"\"\"\n",
    "        Implement core validation function.\n",
    "        \"\"\"\n",
    "        # code here\n",
    "        pass\n",
    "        \n",
    "    def train_batch(self, batch_idx, batch):\n",
    "        \"\"\"\n",
    "        Train a single batch.\n",
    "        \n",
    "        :param batch_idx: current batch index within a given epoch\n",
    "        :param batch: a tuple containing X and y for a single batch\n",
    "        \"\"\"\n",
    "        X, y = batch\n",
    "        # code here\n",
    "        output = self.model(X)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        return loss\n",
    "    \n",
    "    def test_batch(self, batch_idx, batch):\n",
    "        \"\"\"\n",
    "        Test a single batch\n",
    "        \n",
    "        :param batch_idx: current batch index within a given epoch\n",
    "        :param batch: a tuple containing X and y for a single batch\n",
    "        \"\"\"\n",
    "        X, y = batch\n",
    "        # code here\n",
    "        output = self.model(X)\n",
    "        test_loss = F.nll_loss(output, y, reduction='sum').item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        \n",
    "        return test_loss, pred\n",
    "    \n",
    "    def dev_batch(batch_idx, batch):\n",
    "        \"\"\"\n",
    "        Implement a function to process a single batch in dev set.\n",
    "        \n",
    "        :param batch_idx: current batch index within a given epoch\n",
    "        :param batch: a tuple containing X and y for a single batch\n",
    "        \"\"\"\n",
    "        # code here\n",
    "        pass\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Performs one training and testing session over every epoch \n",
    "        until self.max_epochs.\n",
    "        \"\"\"\n",
    "        \n",
    "        # implement early stopping using dev set here.\n",
    "        for epoch in range(self.max_epochs):\n",
    "            self.train(epoch)\n",
    "            self.test(epoch)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Generate predicted labels for the given input data in X.\n",
    "        \n",
    "        :param X: input data\n",
    "        \n",
    "        :return pred: return predicted labels as a tensor 1d array.\n",
    "        \"\"\"\n",
    "        # code here\n",
    "        output = model(X)\n",
    "        test_loss += F.nll_loss(output, y, reduction='sum').item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training and testing sessions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "mean = 0.1307 # calculate the global mean of MNIST images\n",
    "std = 0.3081 # calculate the global std of MNIST images\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    '../data', \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean,), (std,))\n",
    "    ])\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    '../data', \n",
    "    train=False, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean,), (std,))\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64, \n",
    "    shuffle=True, \n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "dev_loader = None # build dev set loader here\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64, \n",
    "    shuffle=True, \n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "tf = TrainingFramework(train_loader,\n",
    "                       test_loader,\n",
    "                       live_plotter=LivePlotter(\"Plottn' Live\", \n",
    "                                                plot_every_n_sec=1),\n",
    "                       max_epochs=100,\n",
    "                       lr=0.2)\n",
    "\n",
    "# start the training.\n",
    "tf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Pick hyperparameter values for grid search.\n",
    "\n",
    "Make use of code already demonstrated in the starter code to extend the pipeline to now evaluate hyperparameters using validation set. Separate random 10k samples from the training set for the validation set. Use the following hyperparameters for your model. Pick any 3 values for each.\n",
    "\n",
    "* **Learning rate** (e.g., 0.2, 0.02, 0.002) \n",
    "* **Early stopping** (e.g., 2, 10, 20 epochs) \n",
    "* **Activation function** (e.g., ReLU, sigmoid, tanh, linear, Leaky ReLU) \n",
    "* **Any model hyperparameter** - Pick any model related hyperparameter from this list : number of convolution layers, changes to the convolution architecture, dropout rate, number of linear layers, different pooling strategies etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate all model variations\n",
    "\n",
    "Perform a simple grid search using any 3 values for each of the hyperparameters. You should be exploring 12 (4x3) variations of your model against the validation set.\n",
    "\n",
    "* Evaluate these 12 different models based on accuracy and report the best model.\n",
    "* Plot all 12 accuracy curves (against the validation set over training epochs) in one image, what do you observe ?\n",
    "* Pick the best model. Report which set of hyperparameters worked the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "Please find sources that provide description of various CNNs and other networks\n",
    "1. https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
    "2. https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d\n",
    "3. https://medium.com/datadriveninvestor/five-powerful-cnn-architectures-b939c9ddd57b\n",
    "4. Neural network playground : https://playground.tensorflow.org/\n",
    "5. Alexnet : http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "6. ResNet: https://arxiv.org/abs/1512.03385\n",
    "7. InceptionNet-v3: https://arxiv.org/abs/1512.00567\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr size=\"6\" style=\"color:#d65828;background-color:#d65828;border:none;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
