{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![osu logo](./images/osu-32px-horiz.png)\n",
    "\n",
    "<hr size=\"6\" style=\"color:#d65828;background-color:#d65828;border:none;\">\n",
    "\n",
    "# Project: Deep Learning\n",
    "\n",
    "This laboratory assignment was designed by Chaitanya Kulkarni and Raghu Machiraju.\n",
    "\n",
    "Due Date: April 9, 2020, 23:59:59. No late submissions allowed unless the Night King from Game of Thrones arises again and invades our campus. Please consult instructor if you are falling behind.\n",
    "\n",
    "Some salient points:\n",
    "* The total number of points is 100 (10% of total grade).\n",
    "* The distribution for each part and question is listed below.\n",
    "* Teams of two/three enrolled students will complete this project.\n",
    "* Discussion across teams is  only permitted on piazza.\n",
    "* Teams are forbidden to exchange code.\n",
    "* If any code is found and used, it should be mentioned in the notebook.\n",
    "* Submission will be through Carmen and we will accept only Jupyter notebooks.\n",
    "* It is required that the notebooks are zipped up and deposited. \n",
    "* The zip file should be named in the following way lastname1-lastame2-labN. (1<=N<=4).\n",
    "* Please please do not include the data with the .zip file.\n",
    "* It is very important that the class follows the honor code of academic conduct  to maximize learning in an open manner.\n",
    "\n",
    ">### _Objective of this Lab:_\n",
    "> * **(100 pts) Build an ANN:** Realize a simple, n layer artificial neural network with both forward and backward propagation and understand how to train, visualize and evaluate the results of the network on mnist dataset.\n",
    ">\n",
    "> * **(100 pts) Build your best CNN:** Use pytorch to construct your own CNN architectures using pytorch modules. If you plan to use any paper’s implementation, be sure to reference those papers. Compare them against your ANN architecture. Compete against your teammates. Top 3 get ….\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Build an Artificial Neural Network \n",
    "\n",
    "You will need to build and train an ANN from scratch to predict the 10 class labels from N image points in a 784 dimensional space (note that each is image 28x28 or 784 pixels). Each pixel will take a value ranging from 0-255. \n",
    "\n",
    "Boilerplate starter code will be provided, as before you only have to fill in the empty code blocks.\n",
    "\n",
    "We breakdown the process into 3 steps.\n",
    "\n",
    "# Step 1: Build the Hidden Layer\n",
    "\n",
    "Create a hidden layer class that:\n",
    "1. Stores the weights.\n",
    "2. Define a forward function to resolve feed forward equations.\n",
    "3. Any activation functions you want to support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer:\n",
    "    def __init__(self, weights_shape, bias=True, activation=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        Defines a HiddenLayer that performs a linear transformation\n",
    "        with the learnt weights. Supports a collection of activation functions.\n",
    "        \n",
    "        :param weights_shape: shape of the weights defined as (in_features, out_features).\n",
    "        :param bias: whether or not to use the bias term in feed forward calculation.\n",
    "        :param activation: activation function to use.\n",
    "        \"\"\"\n",
    "        #code here\n",
    "        \n",
    "    def sigmoid(self, x, d=False):\n",
    "        \"\"\"\n",
    "        The sigmoid activation function with support for derviatives. \n",
    "        \"\"\"\n",
    "        # code here\n",
    "        if d:\n",
    "            # return derivative of sigmoid\n",
    "        \n",
    "        # return sigmoid of x\n",
    "    \n",
    "    def tanh(self, x, d=False):\n",
    "        \"\"\"\n",
    "        The tanh activation function with support for derviatives.\n",
    "        \"\"\"\n",
    "        #code here\n",
    "        \n",
    "        if d:\n",
    "            # return derivative of tanh\n",
    "        \n",
    "        # return tanh of x.\n",
    "        pass\n",
    "    \n",
    "    # define more activation functions here (ReLU, linear, etc)\n",
    "            \n",
    "    def init_weights(self, weights_shape):\n",
    "        \"\"\"\n",
    "        Defines how to initialize the weights. Random or all zeros.\n",
    "        :param weights_shape: the dimensions of the weight matrix to initialize.\n",
    "        \"\"\"\n",
    "        # code here\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Implement the feed forward equation here. \n",
    "        Be sure to store the input and output as those will be \n",
    "        needed in back propogation part.\n",
    "        \n",
    "        :param X: Input of size (m, d), \n",
    "            where m is the batch size, and d is the input feature dim \n",
    "            coming from the previous layer.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.out = # calculate feed forward for the given layer.\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Build the Network with Back Propagation \n",
    "\n",
    "Use **_Mean Squared Error_** as your loss/cost function. (Bonus) Also try **_Cross Entropy_**\n",
    "\n",
    "To support back propogation, revisit the following puesdo code:\n",
    "\n",
    "> #### &emsp;For every mini-batch $X$ of size $m$:\n",
    ">\n",
    "> &emsp;&emsp;1. Generate the Output error $\\delta_L$ of shape $(m, L)$:\n",
    ">    \n",
    "> &emsp;&emsp;&emsp;&emsp;$\\delta_L = \\Delta \\odot \\sigma^\\prime_L(z_L)$\n",
    ">    \n",
    "> &emsp;&emsp;2. For every remaining layers $(L - 1, L - 2, ... 1)$ calculate $\\delta_l$ of shape $(m, l)$:\n",
    ">    \n",
    "> &emsp;&emsp;&emsp;&emsp;$\\delta_l = (w_{l+1} \\cdot \\delta_{l+1}) \\odot \\sigma^\\prime_l(z_l)$\n",
    ">    \n",
    "> &emsp;&emsp;3. Perform gradient descent by updating the weights and bias of all the layers:\n",
    ">    \n",
    "> &emsp;&emsp;&emsp;&emsp;$w_l \\rightarrow w_l - \\frac{\\eta}{m} \\sum_{x}\\delta_{x,l}a_{x,l-1}^T$\n",
    ">    \n",
    "> &emsp;&emsp;&emsp;&emsp;$b_l \\rightarrow b_l - \\frac{\\eta}{m} \\sum_{x}\\delta_{x,l}$\n",
    "\n",
    "Where,\n",
    "\n",
    "$m$ - Batch size\n",
    "\n",
    "$x, X$ - are a single sample and a batch of samples respectively\n",
    "\n",
    "$\\Delta$ - Is the cost function's derivative\n",
    "\n",
    "$L$ - Is the last layer\n",
    "\n",
    "$\\eta$ - learning rate\n",
    "\n",
    "$\\sigma$ - activation function\n",
    "\n",
    "$\\odot$ - [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))\n",
    "\n",
    "$a, z, b, w$ - are defined by the given layer $l$'s feed forward equation: $z_{l+1} = w_{l+1} \\cdot a_l + b_{l+1}$ \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "class ANN:\n",
    "    def __init__(self, arch, lr, input_shape):\n",
    "        \"\"\"\n",
    "        Build the ANN network based on the architecture specified.\n",
    "        \n",
    "        \n",
    "        :param arch: `arch` is a list of values specifying the number \n",
    "            of nodes in each hidden layer. \n",
    "            ie arch = [200, 100, 10] will create a 3-layer network with the \n",
    "            first layer containing 200 nodes, second 100, and the final 10.\n",
    "        :param lr: the learning rate\n",
    "        :input_shape: A tuple that contains the expected input shape. \n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.layers = []\n",
    "        self.lr = lr\n",
    "        self.build(arch)\n",
    "            \n",
    "    def build(self, arch):\n",
    "        \"\"\"\n",
    "        Builds the network arch and store the hidden layers in self.layers.\n",
    "        \n",
    "        :param arch: specify the number of nodes in each hidden \n",
    "            layer as a list where the length is the number of layers.\n",
    "            \n",
    "        :return : none\n",
    "        \"\"\"\n",
    "        for nb_nodes in arch:\n",
    "            \n",
    "            # code here\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Run the input through each layer's forward implementation.\n",
    "        Return the final layer's output.\n",
    "        \n",
    "        :param X: Input as a matrix of size (m, 784)\n",
    "        \n",
    "        :return out: final layers output\n",
    "        \"\"\"\n",
    "        # code here\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        Implement the back propogation algorithm here. Make use of \n",
    "        each layer's stored input's and outputs to update each layer's weights and biases.\n",
    "        \n",
    "        :param y: target labels as a matrix of size (m, 10).\n",
    "        \n",
    "        :return : none\n",
    "        \"\"\"\n",
    "        \n",
    "        # code here\n",
    "            \n",
    "    def mse(self, y_pred, y, d=False):\n",
    "        \"\"\"\n",
    "        Calculate the loss/cost function. Also define its derivative.\n",
    "        :param y_pred: last layer's output after feed forward. Matrix of size (m, 10)\n",
    "        :param y: target labels as a matrix of size (m, 10)\n",
    "        \n",
    "        :return mse: return either the cost function, or its derivative.\n",
    "        \"\"\"\n",
    "        \n",
    "        # code here\n",
    "        \n",
    "        \n",
    "    def train(self, train_loader):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        :param train_loader: an iterator that generates mini-batches for train data\n",
    "        \n",
    "        :return all_loss: a list of loss values per mini-batch. (to plot)\n",
    "        \"\"\"\n",
    "        all_loss = []\n",
    "        for X, y in train_loader:\n",
    "            # code here\n",
    "            \n",
    "        return all_loss\n",
    "    \n",
    "    def test(self, test_loader):\n",
    "        \"\"\"\n",
    "        Test the model. \n",
    "        You can also return the accuracy scores here to see \n",
    "        if the model is training over each epoch.\n",
    "        \n",
    "        :param test_loader: an iterator that generates mini-batches for test data\n",
    "        \n",
    "        :return all_loss: a list of loss values per mini-batch. (to plot)\n",
    "        :return acc: accuracy score for the full 10k test dataset. (to plot)\n",
    "        \"\"\"\n",
    "        loss = []\n",
    "        for X, y in test_loader:\n",
    "            # code here\n",
    "            \n",
    "        return loss, acc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train on MNIST Dataset\n",
    "\n",
    "MNIST PreProcessed Dataset https://www.kaggle.com/oddrationale/mnist-in-csv.\n",
    "\n",
    "We shall use the `mnist_train.csv` (60k) for training, and `mnist_test.csv` (10k) for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv(\"data/mnist_train.csv\")\n",
    "train_y = train_df['label'].to_numpy()\n",
    "train_X = train_df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "test_df = pd.read_csv(\"data/mnist_test.csv\")\n",
    "test_y = test_df['label'].to_numpy()\n",
    "test_X = test_df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "def get_batch(df, n=64):\n",
    "    y = pd.get_dummies(df['label']).to_numpy()\n",
    "    X = df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "    for i in range(X.shape[0] // n):\n",
    "        yield X[n*i:n*(i+1)], y[n*i:n*(i+1)]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NB_EPOCHS = 1000\n",
    "\n",
    "# example ANN\n",
    "net = ANN([100, 10], lr=0.002, input_shape=(BATCH_SIZE, train_X.shape[1]))\n",
    "\n",
    "# train and test the model over multiple epoch.\n",
    "for i in range(NB_EPOCHS):\n",
    "    train_loader = get_batch(train_df, n=BATCH_SIZE)\n",
    "    test_loader = get_batch(test_df, n=BATCH_SIZE)\n",
    "    train_loss = net.train(train_loader)\n",
    "    test_loss = net.test(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluate ANN\n",
    "\n",
    "* Generate confusion matrix, and accuracy score for the test set\n",
    "* Update the `ANN` class to now generate live plots of loss function as the model trains.\n",
    "* Now experiment with the following hyper parameters and report your results. \n",
    "    * Network Architecture (number of layers, number of nodes in each layer, weight initializations, etc)\n",
    "    * Batch size\n",
    "    * Learning rate\n",
    "    * (Bonus) Try **_Cross Entropy_** cost function and compare against **_Mean Squared Error_**.\n",
    "* Report your observations. Which set of hyper parameters worked the best. Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Build your Best CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Plotting\n",
    "\n",
    "This class will allow you to display plots as you are training the model. By default it is set to update plots every 20 sec. You are free to make any modifications to this class if you need additional support to display other than just line plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "class LivePlotter:\n",
    "    def __init__(self, title='', plot_every_n_sec=20):\n",
    "        self.data_dict = defaultdict(list)\n",
    "        self.global_step = 0\n",
    "        self.plot_every_n_sec = plot_every_n_sec\n",
    "        self.start_time = time.time()\n",
    "        self.figsize = (7, 5)\n",
    "        self.title = title\n",
    "        \n",
    "    def update_step(self, step):\n",
    "        \"\"\"\n",
    "        increment global step by `step`.\n",
    "        \"\"\"\n",
    "        self.global_step += step\n",
    "        \n",
    "    def update(self, data_type, x):\n",
    "        \"\"\"\n",
    "        Add a datapoint and plot periodically. \n",
    "        Datapoint is added to the data dictionary given the key in `data_type`.\n",
    "        :param data_type:\n",
    "        :param x:\n",
    "        \"\"\"\n",
    "        self.data_dict[data_type].append((x, self.global_step))\n",
    "        \n",
    "        # plot periodically\n",
    "        if time.time() - self.start_time > self.plot_every_n_sec:\n",
    "            self.plot()\n",
    "            self.start_time = time.time()\n",
    "    \n",
    "    def plot(self):\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=self.figsize)\n",
    "        for label, data in self.data_dict.items():\n",
    "            plt.plot(data['x'], data['y'], label=label)\n",
    "        plt.title(self.title)\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('global_step')\n",
    "        plt.legend(loc='center left') # the plot evolves to the right\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Best Model Here\n",
    "\n",
    "Experiment with different types of models you may find in literature, or that you conceived on your own. \n",
    "\n",
    "> **Important Note!**\n",
    "> Be sure to reference any papers/books you used to develop this model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
